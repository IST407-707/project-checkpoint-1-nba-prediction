{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "O8XafXVLEEhL",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# required libraries\n",
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "import random\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-gVkW9J9JnD-"
   },
   "source": [
    "# create a function to scrape team performance for multiple years"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qTNgHDmdl5eO"
   },
   "source": [
    "to get rosters\n",
    "\n",
    "start with the year -\n",
    "\n",
    "https://www.basketball-reference.com/leagues/NBA_YEAR_ratings\n",
    "\n",
    "loop through each team\n",
    "\n",
    "https://www.basketball-reference.com/teams/THIS_TEAM/2014.html\n",
    "\n",
    "get players from roster -\n",
    "note- no per per game so lets use per from previous season\n",
    "\n",
    "player column of roster\n",
    "which contains a link to the players profile\n",
    "https://www.basketball-reference.com/players/p/pendeje02.html\n",
    "\n",
    "<a href=\"/players/p/pendeje02.html\">Jeff Ayres</a>\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2pXS2tHAgmXa",
    "outputId": "37860ceb-6834-41d6-b634-ed6e65572368",
    "tags": []
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[73], line 59\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# seasons = team_roster_base(years = [2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022])\u001b[39;00m\n\u001b[1;32m     58\u001b[0m seasons \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myear_team_link.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 59\u001b[0m team_rosters \u001b[38;5;241m=\u001b[39m get_rosters(seasons)\n",
      "Cell \u001b[0;32mIn[73], line 29\u001b[0m, in \u001b[0;36mget_rosters\u001b[0;34m(team_roster_base)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m index, row \u001b[38;5;129;01min\u001b[39;00m team_roster_base\u001b[38;5;241m.\u001b[39miterrows():\n\u001b[1;32m     28\u001b[0m     roster_url \u001b[38;5;241m=\u001b[39m base_url\u001b[38;5;241m+\u001b[39m\u001b[38;5;28mstr\u001b[39m(row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mteam_dir\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m---> 29\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(random\u001b[38;5;241m.\u001b[39mrandint(\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m9\u001b[39m))\n\u001b[1;32m     30\u001b[0m     html_team \u001b[38;5;241m=\u001b[39m urlopen(roster_url)\n\u001b[1;32m     31\u001b[0m     soup \u001b[38;5;241m=\u001b[39m BeautifulSoup(html_team, features\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlxml\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def team_roster_base(years = [2015]):\n",
    "    team_roster_base = pd.DataFrame(columns = [\"Year\", \"Team\", \"team_dir\"])\n",
    "    # loop through each year\n",
    "    for y in years:\n",
    "        # NBA season to scrape- year is season end so 2015 is 2014-15 season \n",
    "        year = y\n",
    "        url = f\"https://www.basketball-reference.com/leagues/NBA_{year}_ratings.html\"\n",
    "        html = urlopen(url)\n",
    "        soup = BeautifulSoup(html, features=\"lxml\")\n",
    "        table = soup.find('table', attrs={'id':'ratings'})\n",
    "        teams = table.tbody.findAll(\"tr\")\n",
    "        for team in teams: #get team names and links - would get stats here as well\n",
    "            team_name= team.td.string\n",
    "            team_dir = team.td.a.get('href')\n",
    "            team_year={\"Year\": year, \"Team\": team_name, \"team_dir\": team_dir}\n",
    "            team_roster_base = team_roster_base.append(team_year, ignore_index = True)\n",
    "        time.sleep(5)\n",
    "    # export to csv\n",
    "    team_roster_base.to_csv(\"year_team_link.csv\", index=False)\n",
    "    return team_roster_base\n",
    "\n",
    "def get_rosters(team_roster_base):\n",
    "    save_int = 60\n",
    "    save = 0\n",
    "    yearly_rosters = pd.DataFrame(columns = [\"Year\", \"Team\", \"team_dir\", \"Player\", \"player_dir\"])\n",
    "    base_url = \"https://www.basketball-reference.com\"\n",
    "    for index, row in team_roster_base.iterrows():\n",
    "        roster_url = base_url+str(row['team_dir'])\n",
    "        time.sleep(random.randint(3, 9))\n",
    "        html_team = urlopen(roster_url)\n",
    "        soup = BeautifulSoup(html_team, features=\"lxml\")\n",
    "        roster_table = soup.find('table', attrs={'id':'roster'})\n",
    "        players = roster_table.tbody.findAll(\"tr\")\n",
    "        year = [str(row['Year'])]\n",
    "        team = [str(row['Team'])]\n",
    "        team_dir = [str(row['team_dir'])]\n",
    "        for player in players:\n",
    "            player_name = [str(player.td.string)]\n",
    "            player_dir = [str(player.td.a.get('href'))]\n",
    "            team_year_player = pd.DataFrame({\"Year\": year,\n",
    "                                             \"Team\": team, \n",
    "                                             \"team_dir\": team_dir, \n",
    "                                             \"Player\": player_name, \n",
    "                                             \"player_dir\": player_dir})\n",
    "            yearly_rosters = pd.concat([yearly_rosters, team_year_player], ignore_index = True)\n",
    "            save+=1\n",
    "        if save >= save_int:\n",
    "            save = 0\n",
    "            print(len(yearly_rosters))\n",
    "            with open(\"df.pickle\", \"wb\") as file:\n",
    "                pickle.dump(yearly_rosters, file)\n",
    "    with open(\"df.pickle\", \"wb\") as file:\n",
    "                pickle.dump(yearly_rosters, file)\n",
    "    yearly_rosters.to_csv(\"season_rosters.csv\", index=False)\n",
    "    return yearly_rosters\n",
    "\n",
    "# seasons = team_roster_base(years = [2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022])\n",
    "seasons = pd.read_csv(\"year_team_link.csv\")\n",
    "team_rosters = get_rosters(seasons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_pers(season_rosters, filename):\n",
    "    #get player efficiency ratings \n",
    "    progress = 0 \n",
    "    save = 0\n",
    "    per_table = pd.DataFrame(columns = [\"season\", \"team_id\", \"player_dir\", \"per\"])\n",
    "    save_int = 50   \n",
    "    players_list = []\n",
    "    for index, row in season_rosters.iterrows():\n",
    "        progress+= 1\n",
    "        season = str(int(row['Year'])-1)+'-'+row['Year'][2:]\n",
    "        team = row['team_dir'][7:10]\n",
    "        base_url = \"https://www.basketball-reference.com\"\n",
    "        player_url = base_url+str(row['player_dir'])\n",
    "        if row['player_dir'] in players_list:\n",
    "            continue\n",
    "        else:\n",
    "            players_list.append(row['player_dir'])\n",
    "        try:\n",
    "            time.sleep(random.randint(3, 9))\n",
    "            html_player = urlopen(player_url)\n",
    "            player_seasons = []\n",
    "            player_team_ids = []\n",
    "            # player_team_dirs = []\n",
    "            player_dirs = []\n",
    "            player_pers = []\n",
    "            soup = BeautifulSoup(html_player, features=\"lxml\")\n",
    "            adv_table = soup.find('table', attrs={'id':'advanced'})\n",
    "            teams = adv_table.tbody.findAll(\"tr\")\n",
    "            for team in teams:\n",
    "                # th is season td is all other stats \n",
    "                player_seasons.append(str(team.th.a.string))\n",
    "                print(player_seasons)\n",
    "                other_stats = team.findAll('td')\n",
    "                team_id = other_stats[1].string\n",
    "                player_team_ids.append(str(team_id))\n",
    "                # player_team_dirs.append(row['team_dir'])\n",
    "                player_dirs.append(row['player_dir'])\n",
    "                per_value = other_stats[6].string\n",
    "                player_pers.append(per_value)\n",
    "            this_player = pd .DataFrame({ \"season\": player_seasons,\n",
    "                                         \"team_id\": player_team_ids, \n",
    "                                         \"player_dir\": player_dirs, \n",
    "                                         \"per\":player_pers})\n",
    "            per_table = pd.concat([per_table, this_player], ignore_index = True)\n",
    "            save+=1\n",
    "        except:\n",
    "            print(row)\n",
    "        if save >= save_int:\n",
    "            save = 0\n",
    "            print(len(per_table), end = \" \")\n",
    "#changing filenames for second run \n",
    "            per_table.to_parquet(f'{filename}.parquet.gzip',\n",
    "              compression='gzip')\n",
    "    # https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.to_parquet.html\n",
    "    per_table.to_parquet(f'{filename}.parquet.gzip',\n",
    "              compression='gzip')\n",
    "        \n",
    "    # per_table.to_csv(\"PER_table.csv\", index=False)\n",
    "    return per_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'df.pickle'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# players on our rosters\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m season_rosters \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_pickle\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdf.pickle\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43minfer\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# pers = get_pers(season_rosters)\u001b[39;00m\n\u001b[1;32m      4\u001b[0m season_rosters_players \u001b[38;5;241m=\u001b[39m season_rosters\u001b[38;5;241m.\u001b[39mdrop_duplicates(subset \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mplayer_dir\u001b[39m\u001b[38;5;124m\"\u001b[39m], keep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfirst\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/io/pickle.py:185\u001b[0m, in \u001b[0;36mread_pickle\u001b[0;34m(filepath_or_buffer, compression, storage_options)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;124;03mLoad pickled pandas object (or any object) from file.\u001b[39;00m\n\u001b[1;32m    125\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[38;5;124;03m4    4    9\u001b[39;00m\n\u001b[1;32m    183\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    184\u001b[0m excs_to_catch \u001b[38;5;241m=\u001b[39m (\u001b[38;5;167;01mAttributeError\u001b[39;00m, \u001b[38;5;167;01mImportError\u001b[39;00m, \u001b[38;5;167;01mModuleNotFoundError\u001b[39;00m, \u001b[38;5;167;01mTypeError\u001b[39;00m)\n\u001b[0;32m--> 185\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    186\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    187\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    188\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    189\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    190\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    191\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m handles:\n\u001b[1;32m    192\u001b[0m     \u001b[38;5;66;03m# 1) try standard library Pickle\u001b[39;00m\n\u001b[1;32m    193\u001b[0m     \u001b[38;5;66;03m# 2) try pickle_compat (older pandas version) to handle subclass changes\u001b[39;00m\n\u001b[1;32m    194\u001b[0m     \u001b[38;5;66;03m# 3) try pickle_compat with latin-1 encoding upon a UnicodeDecodeError\u001b[39;00m\n\u001b[1;32m    196\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    197\u001b[0m         \u001b[38;5;66;03m# TypeError for Cython complaints about object.__new__ vs Tick.__new__\u001b[39;00m\n\u001b[1;32m    198\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/io/common.py:882\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[1;32m    874\u001b[0m             handle,\n\u001b[1;32m    875\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    878\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    879\u001b[0m         )\n\u001b[1;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m--> 882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    883\u001b[0m     handles\u001b[38;5;241m.\u001b[39mappend(handle)\n\u001b[1;32m    885\u001b[0m \u001b[38;5;66;03m# Convert BytesIO or file objects passed with an encoding\u001b[39;00m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'df.pickle'"
     ]
    }
   ],
   "source": [
    "# players on our rosters\n",
    "season_rosters = pd.read_pickle(\"df.pickle\", compression='infer')\n",
    "# pers = get_pers(season_rosters)\n",
    "season_rosters_players = season_rosters.drop_duplicates(subset = [\"player_dir\"], keep = \"first\")\n",
    "players_all = season_rosters_players[\"player_dir\"]\n",
    "len(players_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Unable to find a usable engine; tried using: 'pyarrow', 'fastparquet'.\nA suitable version of pyarrow or fastparquet is required for parquet support.\nTrying to import the above resulted in these errors:\n - Missing optional dependency 'pyarrow'. pyarrow is required for parquet support. Use pip or conda to install pyarrow.\n - Missing optional dependency 'fastparquet'. fastparquet is required for parquet support. Use pip or conda to install fastparquet.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#player pers every season up to the error\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m season_PER \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_parquet\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mPER_table.parquet.gzip\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \n\u001b[1;32m      3\u001b[0m season_PER\u001b[38;5;241m=\u001b[39m season_PER\u001b[38;5;241m.\u001b[39mdrop_duplicates(subset \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mplayer_dir\u001b[39m\u001b[38;5;124m\"\u001b[39m], keep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfirst\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m players_have \u001b[38;5;241m=\u001b[39m season_PER[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mplayer_dir\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/io/parquet.py:651\u001b[0m, in \u001b[0;36mread_parquet\u001b[0;34m(path, engine, columns, storage_options, use_nullable_dtypes, dtype_backend, filesystem, filters, **kwargs)\u001b[0m\n\u001b[1;32m    498\u001b[0m \u001b[38;5;129m@doc\u001b[39m(storage_options\u001b[38;5;241m=\u001b[39m_shared_docs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstorage_options\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    499\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mread_parquet\u001b[39m(\n\u001b[1;32m    500\u001b[0m     path: FilePath \u001b[38;5;241m|\u001b[39m ReadBuffer[\u001b[38;5;28mbytes\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    508\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    509\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame:\n\u001b[1;32m    510\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    511\u001b[0m \u001b[38;5;124;03m    Load a parquet object from the file path, returning a DataFrame.\u001b[39;00m\n\u001b[1;32m    512\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    648\u001b[0m \u001b[38;5;124;03m    1    4    9\u001b[39;00m\n\u001b[1;32m    649\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 651\u001b[0m     impl \u001b[38;5;241m=\u001b[39m \u001b[43mget_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    653\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m use_nullable_dtypes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mno_default:\n\u001b[1;32m    654\u001b[0m         msg \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    655\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe argument \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124muse_nullable_dtypes\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is deprecated and will be removed \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    656\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124min a future version.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    657\u001b[0m         )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/io/parquet.py:67\u001b[0m, in \u001b[0;36mget_engine\u001b[0;34m(engine)\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m     65\u001b[0m             error_msgs \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m - \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(err)\n\u001b[0;32m---> 67\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[1;32m     68\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnable to find a usable engine; \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     69\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtried using: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpyarrow\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfastparquet\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     70\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA suitable version of \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     71\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpyarrow or fastparquet is required for parquet \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     72\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msupport.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     73\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrying to import the above resulted in these errors:\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     74\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00merror_msgs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     75\u001b[0m     )\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m engine \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpyarrow\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m     78\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m PyArrowImpl()\n",
      "\u001b[0;31mImportError\u001b[0m: Unable to find a usable engine; tried using: 'pyarrow', 'fastparquet'.\nA suitable version of pyarrow or fastparquet is required for parquet support.\nTrying to import the above resulted in these errors:\n - Missing optional dependency 'pyarrow'. pyarrow is required for parquet support. Use pip or conda to install pyarrow.\n - Missing optional dependency 'fastparquet'. fastparquet is required for parquet support. Use pip or conda to install fastparquet."
     ]
    }
   ],
   "source": [
    "#player pers every season up to the error\n",
    "season_PER = pd.read_parquet('PER_table.parquet.gzip') \n",
    "season_PER= season_PER.drop_duplicates(subset = [\"player_dir\"], keep = \"first\")\n",
    "players_have = season_PER[\"player_dir\"]\n",
    "players_have"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "140"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#players missing due to the error\n",
    "players_needed = set(players_all).difference(set(players_have))\n",
    "len(set(players_all).difference(set(players_have)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Year</th>\n",
       "      <th>Team</th>\n",
       "      <th>team_dir</th>\n",
       "      <th>Player</th>\n",
       "      <th>player_dir</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4012</th>\n",
       "      <td>2021</td>\n",
       "      <td>Houston Rockets</td>\n",
       "      <td>/teams/HOU/2021.html</td>\n",
       "      <td>Brodric Thomas</td>\n",
       "      <td>/players/t/thomabr01.html</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4034</th>\n",
       "      <td>2021</td>\n",
       "      <td>Cleveland Cavaliers</td>\n",
       "      <td>/teams/CLE/2021.html</td>\n",
       "      <td>Isaac Okoro</td>\n",
       "      <td>/players/o/okorois01.html</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4038</th>\n",
       "      <td>2021</td>\n",
       "      <td>Cleveland Cavaliers</td>\n",
       "      <td>/teams/CLE/2021.html</td>\n",
       "      <td>Lamar Stevens</td>\n",
       "      <td>/players/s/stevela01.html</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4042</th>\n",
       "      <td>2021</td>\n",
       "      <td>Cleveland Cavaliers</td>\n",
       "      <td>/teams/CLE/2021.html</td>\n",
       "      <td>Dylan Windler</td>\n",
       "      <td>/players/w/windldy01.html</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4044</th>\n",
       "      <td>2021</td>\n",
       "      <td>Orlando Magic</td>\n",
       "      <td>/teams/ORL/2021.html</td>\n",
       "      <td>Cole Anthony</td>\n",
       "      <td>/players/a/anthoco01.html</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4783</th>\n",
       "      <td>2022</td>\n",
       "      <td>Portland Trail Blazers</td>\n",
       "      <td>/teams/POR/2022.html</td>\n",
       "      <td>Greg Brown III</td>\n",
       "      <td>/players/b/browngr01.html</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4785</th>\n",
       "      <td>2022</td>\n",
       "      <td>Portland Trail Blazers</td>\n",
       "      <td>/teams/POR/2022.html</td>\n",
       "      <td>Jarron Cumberland</td>\n",
       "      <td>/players/c/cumbeja01.html</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4796</th>\n",
       "      <td>2022</td>\n",
       "      <td>Portland Trail Blazers</td>\n",
       "      <td>/teams/POR/2022.html</td>\n",
       "      <td>Cameron McGriff</td>\n",
       "      <td>/players/m/mcgrica01.html</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4805</th>\n",
       "      <td>2022</td>\n",
       "      <td>Portland Trail Blazers</td>\n",
       "      <td>/teams/POR/2022.html</td>\n",
       "      <td>Trendon Watford</td>\n",
       "      <td>/players/w/watfotr01.html</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4806</th>\n",
       "      <td>2022</td>\n",
       "      <td>Portland Trail Blazers</td>\n",
       "      <td>/teams/POR/2022.html</td>\n",
       "      <td>Brandon Williams</td>\n",
       "      <td>/players/w/willibr03.html</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>140 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Year                    Team              team_dir             Player  \\\n",
       "4012  2021         Houston Rockets  /teams/HOU/2021.html     Brodric Thomas   \n",
       "4034  2021     Cleveland Cavaliers  /teams/CLE/2021.html        Isaac Okoro   \n",
       "4038  2021     Cleveland Cavaliers  /teams/CLE/2021.html      Lamar Stevens   \n",
       "4042  2021     Cleveland Cavaliers  /teams/CLE/2021.html      Dylan Windler   \n",
       "4044  2021           Orlando Magic  /teams/ORL/2021.html       Cole Anthony   \n",
       "...    ...                     ...                   ...                ...   \n",
       "4783  2022  Portland Trail Blazers  /teams/POR/2022.html     Greg Brown III   \n",
       "4785  2022  Portland Trail Blazers  /teams/POR/2022.html  Jarron Cumberland   \n",
       "4796  2022  Portland Trail Blazers  /teams/POR/2022.html    Cameron McGriff   \n",
       "4805  2022  Portland Trail Blazers  /teams/POR/2022.html    Trendon Watford   \n",
       "4806  2022  Portland Trail Blazers  /teams/POR/2022.html   Brandon Williams   \n",
       "\n",
       "                     player_dir  \n",
       "4012  /players/t/thomabr01.html  \n",
       "4034  /players/o/okorois01.html  \n",
       "4038  /players/s/stevela01.html  \n",
       "4042  /players/w/windldy01.html  \n",
       "4044  /players/a/anthoco01.html  \n",
       "...                         ...  \n",
       "4783  /players/b/browngr01.html  \n",
       "4785  /players/c/cumbeja01.html  \n",
       "4796  /players/m/mcgrica01.html  \n",
       "4805  /players/w/watfotr01.html  \n",
       "4806  /players/w/willibr03.html  \n",
       "\n",
       "[140 rows x 5 columns]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#selecting the players we dont have \n",
    "missing_players = season_rosters_players[season_rosters_players[\"player_dir\"].isin(players_needed)]\n",
    "len(missing_players)\n",
    "missing_players"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Year                               2022\n",
      "Team                    Milwaukee Bucks\n",
      "team_dir           /teams/MIL/2022.html\n",
      "Player                     Luca Vildoza\n",
      "player_dir    /players/v/vildolu01.html\n",
      "Name: 4285, dtype: object\n",
      "151 276 "
     ]
    }
   ],
   "source": [
    "# scraping the missing players into missing pers\n",
    "missing_players_df = get_pers(missing_players, \"missing_players\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# players after scraping attempt for missing \n",
    "mp = pd.read_parquet('missing_players.parquet.gzip') \n",
    "mp= mp.drop_duplicates(subset = [\"player_dir\"], keep = \"first\")\n",
    "players_mp = mp[\"player_dir\"]\n",
    "players_mp\n",
    "#merge with existing\n",
    "players_have = pd.concat([players_have, players_mp], ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#players still missing\n",
    "players_needed = set(players_all).difference(set(players_have))\n",
    "len(set(players_all).difference(set(players_have)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Year</th>\n",
       "      <th>Team</th>\n",
       "      <th>team_dir</th>\n",
       "      <th>Player</th>\n",
       "      <th>player_dir</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4285</th>\n",
       "      <td>2022</td>\n",
       "      <td>Milwaukee Bucks</td>\n",
       "      <td>/teams/MIL/2022.html</td>\n",
       "      <td>Luca Vildoza</td>\n",
       "      <td>/players/v/vildolu01.html</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Year             Team              team_dir        Player  \\\n",
       "4285  2022  Milwaukee Bucks  /teams/MIL/2022.html  Luca Vildoza   \n",
       "\n",
       "                     player_dir  \n",
       "4285  /players/v/vildolu01.html  "
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#selecting the player we dont have \n",
    "missing_players = season_rosters_players[season_rosters_players[\"player_dir\"].isin(players_needed)]\n",
    "len(missing_players)\n",
    "missing_players"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>season</th>\n",
       "      <th>team_id</th>\n",
       "      <th>player_dir</th>\n",
       "      <th>per</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2014-15</td>\n",
       "      <td>GSW</td>\n",
       "      <td>/players/b/barbole01.html</td>\n",
       "      <td>15.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2015-16</td>\n",
       "      <td>GSW</td>\n",
       "      <td>/players/b/barbole01.html</td>\n",
       "      <td>11.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2016-17</td>\n",
       "      <td>PHO</td>\n",
       "      <td>/players/b/barbole01.html</td>\n",
       "      <td>11.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2014-15</td>\n",
       "      <td>GSW</td>\n",
       "      <td>/players/b/barneha02.html</td>\n",
       "      <td>13.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2015-16</td>\n",
       "      <td>GSW</td>\n",
       "      <td>/players/b/barneha02.html</td>\n",
       "      <td>12.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9245</th>\n",
       "      <td>2021-22</td>\n",
       "      <td>POR</td>\n",
       "      <td>/players/b/browngr01.html</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9248</th>\n",
       "      <td>2021-22</td>\n",
       "      <td>POR</td>\n",
       "      <td>/players/c/cumbeja01.html</td>\n",
       "      <td>8.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9249</th>\n",
       "      <td>2021-22</td>\n",
       "      <td>POR</td>\n",
       "      <td>/players/m/mcgrica01.html</td>\n",
       "      <td>12.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9250</th>\n",
       "      <td>2021-22</td>\n",
       "      <td>POR</td>\n",
       "      <td>/players/w/watfotr01.html</td>\n",
       "      <td>15.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9253</th>\n",
       "      <td>2021-22</td>\n",
       "      <td>POR</td>\n",
       "      <td>/players/w/willibr03.html</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5143 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       season team_id                 player_dir   per\n",
       "13    2014-15     GSW  /players/b/barbole01.html  15.3\n",
       "14    2015-16     GSW  /players/b/barbole01.html  11.7\n",
       "15    2016-17     PHO  /players/b/barbole01.html  11.5\n",
       "18    2014-15     GSW  /players/b/barneha02.html  13.4\n",
       "19    2015-16     GSW  /players/b/barneha02.html  12.3\n",
       "...       ...     ...                        ...   ...\n",
       "9245  2021-22     POR  /players/b/browngr01.html  11.0\n",
       "9248  2021-22     POR  /players/c/cumbeja01.html   8.4\n",
       "9249  2021-22     POR  /players/m/mcgrica01.html  12.5\n",
       "9250  2021-22     POR  /players/w/watfotr01.html  15.8\n",
       "9253  2021-22     POR  /players/w/willibr03.html  11.0\n",
       "\n",
       "[5143 rows x 4 columns]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "luca_vildoza = pd.DataFrame({ \"season\": [2021-22],\n",
    "                                         \"team_id\": [\"MIL\"], \n",
    "                                         \"player_dir\": [\"/players/v/vildolu01.html\"], \n",
    "                                         \"per\":[17.9]})\n",
    "season_player_per = pd.concat([pd.read_parquet('PER_table.parquet.gzip'), pd.read_parquet('missing_players.parquet.gzip') , luca_vildoza], ignore_index = True)\n",
    "# season_player_per.drop_duplicates(subset = [\"player_dir\"], keep = \"first\")\n",
    "# season_player_per.to_csv(\"seasons_per_s.csv\", index=False)\n",
    "#need to remove season pers before the year 2015\n",
    "season_player_per = season_player_per[season_player_per[\"season\"].isin([\"2014-15\", \"2015-16\", \"2016-17\", \"2017-18\", \"2018-19\", \"2019-20\", \"2020-21\", \"2021-22\"])]\n",
    "#this list is larger because of trades/transfers etc\n",
    "season_player_per.to_csv(\"seasons_per_s.csv\", index=False)\n",
    "season_player_per"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Year</th>\n",
       "      <th>Team</th>\n",
       "      <th>team_dir</th>\n",
       "      <th>Player</th>\n",
       "      <th>player_dir</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2015</td>\n",
       "      <td>Golden State Warriors</td>\n",
       "      <td>/teams/GSW/2015.html</td>\n",
       "      <td>Leandro Barbosa</td>\n",
       "      <td>/players/b/barbole01.html</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2015</td>\n",
       "      <td>Golden State Warriors</td>\n",
       "      <td>/teams/GSW/2015.html</td>\n",
       "      <td>Harrison Barnes</td>\n",
       "      <td>/players/b/barneha02.html</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2015</td>\n",
       "      <td>Golden State Warriors</td>\n",
       "      <td>/teams/GSW/2015.html</td>\n",
       "      <td>Andrew Bogut</td>\n",
       "      <td>/players/b/bogutan01.html</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2015</td>\n",
       "      <td>Golden State Warriors</td>\n",
       "      <td>/teams/GSW/2015.html</td>\n",
       "      <td>Stephen Curry</td>\n",
       "      <td>/players/c/curryst01.html</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2015</td>\n",
       "      <td>Golden State Warriors</td>\n",
       "      <td>/teams/GSW/2015.html</td>\n",
       "      <td>Festus Ezeli</td>\n",
       "      <td>/players/e/ezelife01.html</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4804</th>\n",
       "      <td>2022</td>\n",
       "      <td>Portland Trail Blazers</td>\n",
       "      <td>/teams/POR/2022.html</td>\n",
       "      <td>Tony Snell</td>\n",
       "      <td>/players/s/snellto01.html</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4805</th>\n",
       "      <td>2022</td>\n",
       "      <td>Portland Trail Blazers</td>\n",
       "      <td>/teams/POR/2022.html</td>\n",
       "      <td>Trendon Watford</td>\n",
       "      <td>/players/w/watfotr01.html</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4806</th>\n",
       "      <td>2022</td>\n",
       "      <td>Portland Trail Blazers</td>\n",
       "      <td>/teams/POR/2022.html</td>\n",
       "      <td>Brandon Williams</td>\n",
       "      <td>/players/w/willibr03.html</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4807</th>\n",
       "      <td>2022</td>\n",
       "      <td>Portland Trail Blazers</td>\n",
       "      <td>/teams/POR/2022.html</td>\n",
       "      <td>Justise Winslow</td>\n",
       "      <td>/players/w/winslju01.html</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4808</th>\n",
       "      <td>2022</td>\n",
       "      <td>Portland Trail Blazers</td>\n",
       "      <td>/teams/POR/2022.html</td>\n",
       "      <td>Cody Zeller</td>\n",
       "      <td>/players/z/zelleco01.html</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4809 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Year                    Team              team_dir            Player  \\\n",
       "0     2015   Golden State Warriors  /teams/GSW/2015.html   Leandro Barbosa   \n",
       "1     2015   Golden State Warriors  /teams/GSW/2015.html   Harrison Barnes   \n",
       "2     2015   Golden State Warriors  /teams/GSW/2015.html      Andrew Bogut   \n",
       "3     2015   Golden State Warriors  /teams/GSW/2015.html     Stephen Curry   \n",
       "4     2015   Golden State Warriors  /teams/GSW/2015.html      Festus Ezeli   \n",
       "...    ...                     ...                   ...               ...   \n",
       "4804  2022  Portland Trail Blazers  /teams/POR/2022.html        Tony Snell   \n",
       "4805  2022  Portland Trail Blazers  /teams/POR/2022.html   Trendon Watford   \n",
       "4806  2022  Portland Trail Blazers  /teams/POR/2022.html  Brandon Williams   \n",
       "4807  2022  Portland Trail Blazers  /teams/POR/2022.html   Justise Winslow   \n",
       "4808  2022  Portland Trail Blazers  /teams/POR/2022.html       Cody Zeller   \n",
       "\n",
       "                     player_dir  \n",
       "0     /players/b/barbole01.html  \n",
       "1     /players/b/barneha02.html  \n",
       "2     /players/b/bogutan01.html  \n",
       "3     /players/c/curryst01.html  \n",
       "4     /players/e/ezelife01.html  \n",
       "...                         ...  \n",
       "4804  /players/s/snellto01.html  \n",
       "4805  /players/w/watfotr01.html  \n",
       "4806  /players/w/willibr03.html  \n",
       "4807  /players/w/winslju01.html  \n",
       "4808  /players/z/zelleco01.html  \n",
       "\n",
       "[4809 rows x 5 columns]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "season_rosters = pd.read_pickle(\"df.pickle\", compression='infer')\n",
    "season_rosters "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#final dataframes season_per_s.csv and season_rosters.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# next step get team stats- how are we defining team stats \n",
    "#we need team stats for each game player over the regular season by each team. \n",
    "\n",
    "#1 scrape season matchups from https://www.basketball-reference.com/leagues/NBA_2015_games-MONTH.html (OCTOBER- APRIL(PLAYOFFS))\n",
    "months = [\"october\", \"november\", \"december\", \"january\", \"february\", \"march\", \"april\"]\n",
    "f\"https://www.basketball-reference.com/leagues/NBA_2015_games-{month}.html\"\n",
    "for month in month:\n",
    "schedule_table = soup.find('table', attrs={'id':'schedule'})\n",
    "this_game = adv_table.tbody.findAll(\"tr\")\n",
    "####### make sure its before the playoffs row in april \n",
    "save team abreviations from link \"/team/TEAM ID/year.html, score? home or away \n",
    "<!-- ####### save arena, team as home or away score  -->\n",
    "\n",
    "#2 USE BOX SCORE LINK TO GET INDIVIDUAL GAME STAS \n",
    "EX. https://www.basketball-reference.com/boxscores/201504010CHO.html 201504010CHO- DIFFERENT FOR EACH GAME \n",
    "\n",
    "####### use team_id's to identify team stats table from the game this table also has player stats if we decde to use them \n",
    "ex. soup.find('table', attrs={'id':\"box-TEAM_ID-game-basic\"})\n",
    "get team stats from the tfoot tag\n",
    "\n",
    "#3 save data - probably should do this in seasonal increments "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_season_schedule(filename, seasons = [2015]):\n",
    "    season_schedule_table = pd.DataFrame(columns = [\"date\", \"away_team_id\", \"home_team_id\", \"arena\", \"link\"])\n",
    "    months = [\"october\", \"november\", \"december\", \"january\", \"february\", \"march\", \"april\"]\n",
    "    for year in seasons:\n",
    "        for month in months:\n",
    "            month_schedule = f\"https://www.basketball-reference.com/leagues/NBA_{year}_games-{month}.html\"\n",
    "            print(month_schedule)\n",
    "            time.sleep(random.randint(2, 6))\n",
    "            month_schedule_link = urlopen(month_schedule)\n",
    "            soup = BeautifulSoup(month_schedule_link, features=\"lxml\")\n",
    "            schedule_table = soup.find('table', attrs={'id':'schedule'})\n",
    "            games = schedule_table.tbody.findAll(\"tr\")\n",
    "            dates = []\n",
    "            away_team_ids = []\n",
    "            home_team_ids = []\n",
    "            arenas = []\n",
    "            links = []\n",
    "            for game in games:\n",
    "            #avoinding playoffs schedule\n",
    "                if game.th.string == \"Playoffs\":\n",
    "                    season_month = pd.DataFrame({\"date\": dates,\n",
    "                                         \"away_team_id\": away_team_ids, \n",
    "                                         \"home_team_id\": home_team_ids, \n",
    "                                          \"arena\":arenas,\n",
    "                                         \"link\":links\n",
    "                                        })\n",
    "                    break\n",
    "                    \n",
    "                other_stats = game.findAll(\"td\")\n",
    "                date = game.th.a.string\n",
    "                dates.append(date)\n",
    "                away_team = other_stats[1].a.get('href')[7:10]\n",
    "                away_team_ids.append(away_team)\n",
    "                home_team = other_stats[3].a.get('href')[7:10]\n",
    "                home_team_ids.append(home_team)\n",
    "                boxscore = other_stats[5].a.get('href')\n",
    "                links.append(boxscore)\n",
    "                arena = other_stats[8].string\n",
    "                arenas.append(arena)\n",
    "            season_month = pd.DataFrame({\"date\": dates,\n",
    "                                         \"away_team_id\": away_team_ids, \n",
    "                                         \"home_team_id\": home_team_ids, \n",
    "                                          \"arena\":arenas,\n",
    "                                         \"link\":links\n",
    "                                        })\n",
    "            try:\n",
    "                name_of_file = f\"{filename}.parquet.gzip\"\n",
    "                season_games = pd.concat([pd.read_parquet(name_of_file), season_month], ignore_index = True)\n",
    "                season_games.to_parquet(name_of_file)              \n",
    "            except:\n",
    "                name_of_file = f\"{filename}.parquet.gzip\"\n",
    "                season_month.to_parquet(name_of_file)\n",
    "            print(month, \"completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.basketball-reference.com/leagues/NBA_2015_games-october.html\n",
      "october completed\n",
      "https://www.basketball-reference.com/leagues/NBA_2015_games-november.html\n",
      "november completed\n",
      "https://www.basketball-reference.com/leagues/NBA_2015_games-december.html\n",
      "december completed\n",
      "https://www.basketball-reference.com/leagues/NBA_2015_games-january.html\n",
      "january completed\n",
      "https://www.basketball-reference.com/leagues/NBA_2015_games-february.html\n",
      "february completed\n",
      "https://www.basketball-reference.com/leagues/NBA_2015_games-march.html\n",
      "march completed\n",
      "https://www.basketball-reference.com/leagues/NBA_2015_games-april.html\n",
      "april completed\n"
     ]
    }
   ],
   "source": [
    "# 2015 to start \n",
    "# years = [2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022]\n",
    "seasons = [2015]\n",
    "get_season_schedule(str(seasons), seasons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>away_team_id</th>\n",
       "      <th>home_team_id</th>\n",
       "      <th>arena</th>\n",
       "      <th>link</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Tue, Oct 28, 2014</td>\n",
       "      <td>ORL</td>\n",
       "      <td>NOP</td>\n",
       "      <td>Smoothie King Center</td>\n",
       "      <td>/boxscores/201410280NOP.html</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Tue, Oct 28, 2014</td>\n",
       "      <td>DAL</td>\n",
       "      <td>SAS</td>\n",
       "      <td>AT&amp;T Center</td>\n",
       "      <td>/boxscores/201410280SAS.html</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Tue, Oct 28, 2014</td>\n",
       "      <td>HOU</td>\n",
       "      <td>LAL</td>\n",
       "      <td>STAPLES Center</td>\n",
       "      <td>/boxscores/201410280LAL.html</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Wed, Oct 29, 2014</td>\n",
       "      <td>MIL</td>\n",
       "      <td>CHO</td>\n",
       "      <td>Time Warner Cable Arena</td>\n",
       "      <td>/boxscores/201410290CHO.html</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Wed, Oct 29, 2014</td>\n",
       "      <td>PHI</td>\n",
       "      <td>IND</td>\n",
       "      <td>Bankers Life Fieldhouse</td>\n",
       "      <td>/boxscores/201410290IND.html</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2575</th>\n",
       "      <td>Wed, Apr 15, 2015</td>\n",
       "      <td>DET</td>\n",
       "      <td>NYK</td>\n",
       "      <td>Madison Square Garden (IV)</td>\n",
       "      <td>/boxscores/201504150NYK.html</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2576</th>\n",
       "      <td>Wed, Apr 15, 2015</td>\n",
       "      <td>MIA</td>\n",
       "      <td>PHI</td>\n",
       "      <td>Wells Fargo Center</td>\n",
       "      <td>/boxscores/201504150PHI.html</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2577</th>\n",
       "      <td>Wed, Apr 15, 2015</td>\n",
       "      <td>IND</td>\n",
       "      <td>MEM</td>\n",
       "      <td>FedEx Forum</td>\n",
       "      <td>/boxscores/201504150MEM.html</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2578</th>\n",
       "      <td>Wed, Apr 15, 2015</td>\n",
       "      <td>DEN</td>\n",
       "      <td>GSW</td>\n",
       "      <td>Oracle Arena</td>\n",
       "      <td>/boxscores/201504150GSW.html</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2579</th>\n",
       "      <td>Wed, Apr 15, 2015</td>\n",
       "      <td>SAC</td>\n",
       "      <td>LAL</td>\n",
       "      <td>STAPLES Center</td>\n",
       "      <td>/boxscores/201504150LAL.html</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2580 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   date away_team_id home_team_id                       arena  \\\n",
       "0     Tue, Oct 28, 2014          ORL          NOP        Smoothie King Center   \n",
       "1     Tue, Oct 28, 2014          DAL          SAS                 AT&T Center   \n",
       "2     Tue, Oct 28, 2014          HOU          LAL              STAPLES Center   \n",
       "3     Wed, Oct 29, 2014          MIL          CHO     Time Warner Cable Arena   \n",
       "4     Wed, Oct 29, 2014          PHI          IND     Bankers Life Fieldhouse   \n",
       "...                 ...          ...          ...                         ...   \n",
       "2575  Wed, Apr 15, 2015          DET          NYK  Madison Square Garden (IV)   \n",
       "2576  Wed, Apr 15, 2015          MIA          PHI          Wells Fargo Center   \n",
       "2577  Wed, Apr 15, 2015          IND          MEM                 FedEx Forum   \n",
       "2578  Wed, Apr 15, 2015          DEN          GSW                Oracle Arena   \n",
       "2579  Wed, Apr 15, 2015          SAC          LAL              STAPLES Center   \n",
       "\n",
       "                              link  \n",
       "0     /boxscores/201410280NOP.html  \n",
       "1     /boxscores/201410280SAS.html  \n",
       "2     /boxscores/201410280LAL.html  \n",
       "3     /boxscores/201410290CHO.html  \n",
       "4     /boxscores/201410290IND.html  \n",
       "...                            ...  \n",
       "2575  /boxscores/201504150NYK.html  \n",
       "2576  /boxscores/201504150PHI.html  \n",
       "2577  /boxscores/201504150MEM.html  \n",
       "2578  /boxscores/201504150GSW.html  \n",
       "2579  /boxscores/201504150LAL.html  \n",
       "\n",
       "[2580 rows x 5 columns]"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name_of_file = f\"{str(seasons)}.parquet.gzip\"\n",
    "season_games = pd.read_parquet(name_of_file)\n",
    "season_games"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_game_stats(dataframe, filename):\n",
    "    count = 0 \n",
    "    for game in dataframe.iterrows():\n",
    "        game = game[1]\n",
    "        home_team_id = game[\"home_team_id\"]\n",
    "        away_team_id = game[\"away_team_id\"]\n",
    "        arena = game[\"arena\"]\n",
    "        ext = game[\"link\"]\n",
    "        box_score_url = f\"https://www.basketball-reference.com{ext}\"\n",
    "        time.sleep(random.randint(2, 6))\n",
    "        open_link = urlopen(box_score_url)\n",
    "        soup = BeautifulSoup(open_link, features=\"lxml\")\n",
    "        home_stats = soup.find('table', attrs={'id':f\"box-{home_team_id}-game-basic\"}).tfoot.tr\n",
    "        home_fg = home_stats.find('td', attrs = {'data-stat': 'fg'}).string\n",
    "        home_fga = home_stats.find('td', attrs = {'data-stat': 'fga'}).string\n",
    "        home_fg_pct = home_stats.find('td', attrs = {'data-stat': 'fg_pct'}).string\n",
    "        home_fg3 = home_stats.find('td', attrs = {'data-stat': 'fg3'}).string\n",
    "        home_fg3a = home_stats.find('td', attrs = {'data-stat': 'fg3a'}).string\n",
    "        home_fg3_pct = home_stats.find('td', attrs = {'data-stat': 'fg3_pct'}).string\n",
    "        home_ft = home_stats.find('td', attrs = {'data-stat': 'ft'}).string\n",
    "        home_fta = home_stats.find('td', attrs = {'data-stat': 'fta'}).string\n",
    "        home_ft_pct = home_stats.find('td', attrs = {'data-stat': 'ft_pct'}).string\n",
    "        home_orb = home_stats.find('td', attrs = {'data-stat': 'orb'}).string\n",
    "        home_drb = home_stats.find('td', attrs = {'data-stat': 'drb'}).string\n",
    "        home_trb = home_stats.find('td', attrs = {'data-stat': 'trb'}).string\n",
    "        home_ast = home_stats.find('td', attrs = {'data-stat': 'ast'}).string\n",
    "        home_stl = home_stats.find('td', attrs = {'data-stat': 'stl'}).string\n",
    "        home_blk = home_stats.find('td', attrs = {'data-stat': 'blk'}).string\n",
    "        home_tov = home_stats.find('td', attrs = {'data-stat': 'tov'}).string\n",
    "        home_pf = home_stats.find('td', attrs = {'data-stat': 'pf'}).string\n",
    "        home_pts = home_stats.find('td', attrs = {'data-stat': 'pts'}).string\n",
    "        \n",
    "        away_stats = soup.find('table', attrs={'id':f\"box-{away_team_id}-game-basic\"}).tfoot.tr\n",
    "        away_fg = away_stats.find('td', attrs = {'data-stat': 'fg'}).string\n",
    "        away_fga = away_stats.find('td', attrs = {'data-stat': 'fga'}).string\n",
    "        away_fg_pct = away_stats.find('td', attrs = {'data-stat': 'fg_pct'}).string\n",
    "        away_fg3 = away_stats.find('td', attrs = {'data-stat': 'fg3'}).string\n",
    "        away_fg3a = away_stats.find('td', attrs = {'data-stat': 'fg3a'}).string\n",
    "        away_fg3_pct = away_stats.find('td', attrs = {'data-stat': 'fg3_pct'}).string\n",
    "        away_ft = away_stats.find('td', attrs = {'data-stat': 'ft'}).string\n",
    "        away_fta = away_stats.find('td', attrs = {'data-stat': 'fta'}).string\n",
    "        away_ft_pct = away_stats.find('td', attrs = {'data-stat': 'ft_pct'}).string\n",
    "        away_orb = away_stats.find('td', attrs = {'data-stat': 'orb'}).string\n",
    "        away_drb = away_stats.find('td', attrs = {'data-stat': 'drb'}).string\n",
    "        away_trb = away_stats.find('td', attrs = {'data-stat': 'trb'}).string\n",
    "        away_ast = away_stats.find('td', attrs = {'data-stat': 'ast'}).string\n",
    "        away_stl = away_stats.find('td', attrs = {'data-stat': 'stl'}).string\n",
    "        away_blk = away_stats.find('td', attrs = {'data-stat': 'blk'}).string\n",
    "        away_tov = away_stats.find('td', attrs = {'data-stat': 'tov'}).string\n",
    "        away_pf = away_stats.find('td', attrs = {'data-stat': 'pf'}).string\n",
    "        away_pts = away_stats.find('td', attrs = {'data-stat': 'pts'}).string\n",
    "    \n",
    "        game_stats = pd.DataFrame({\n",
    "        \"home_team\":[home_team_id],\n",
    "        \"home_fg\":[home_fg], \n",
    "        \"home_fga\":[home_fga], \n",
    "        \"home_fg_pct\":[home_fg_pct], \n",
    "        \"home_fg3\":[home_fg3], \n",
    "        \"home_fg3a\":[home_fg3a], \n",
    "        \"home_fg3_pct\":[home_fg3_pct], \n",
    "        \"home_ft\":[home_ft], \n",
    "        \"home_fta\":[home_fta], \n",
    "        \"home_ft_pct\":[home_ft_pct], \n",
    "        \"home_orb\":[home_orb], \n",
    "        \"home_drb\":[home_drb],\n",
    "        \"home_trb\":[home_trb],\n",
    "        \"home_ast\":[home_ast],\n",
    "        \"home_stl\":[home_stl],\n",
    "        \"home_blk\":[home_blk],\n",
    "        \"home_tov\":[home_tov],\n",
    "        \"home_pf\":[home_pf],\n",
    "        \"home_pts\":[home_pts],\n",
    "        \"away_team\":[away_team_id],\n",
    "        \"away_fg\":[away_fg], \n",
    "        \"away_fga\":[away_fga], \n",
    "        \"away_fg_pct\":[away_fg_pct], \n",
    "        \"away_fg3\":[away_fg3], \n",
    "        \"away_fg3a\":[away_fg3a], \n",
    "        \"away_fg3_pct\":[away_fg3_pct], \n",
    "        \"away_ft\":[away_ft], \n",
    "        \"away_fta\":[away_fta], \n",
    "        \"away_ft_pct\":[away_ft_pct], \n",
    "        \"away_orb\":[away_orb], \n",
    "        \"away_drb\":[away_drb],\n",
    "        \"away_trb\":[away_trb],\n",
    "        \"away_ast\":[away_ast],\n",
    "        \"away_stl\":[away_stl],\n",
    "        \"away_blk\":[away_blk],\n",
    "        \"away_tov\":[away_tov],\n",
    "        \"away_pf\":[away_pf],\n",
    "        \"away_pts\":[away_pts],\n",
    "        \"arena\": [arena]})\n",
    "        if count % 100 == 0:\n",
    "            print(count)   \n",
    "            try:\n",
    "                name_of_file = f\"{filename}.parquet.gzip\"\n",
    "                game_stats = pd.concat([pd.read_parquet(name_of_file), game_stats], ignore_index = True)\n",
    "                game_stats.to_parquet(name_of_file)              \n",
    "            except:\n",
    "                name_of_file = f\"{filename}.parquet.gzip\"\n",
    "                game_stats.to_parquet(name_of_file)\n",
    "            count+=1\n",
    "    name_of_file = f\"{filename}.parquet.gzip\"\n",
    "    game_stats = pd.concat([pd.read_parquet(name_of_file), game_stats], ignore_index = True)\n",
    "    game_stats.to_parquet(name_of_file) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "get_game_stats(season_games, \"2014-15_game_stats\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# required libraries\n",
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "import random\n",
    "import pickle\n",
    "\n",
    "def get_season_schedule(filename, seasons = [2015]):\n",
    "    '''\n",
    "    Gets the season schedule (games played throughout the season) up until the playoffs. \n",
    "    creates a dataframe to inform us of any potential errors and allows code to not break because of errors\n",
    "    '''\n",
    "    season_schedule_table = pd.DataFrame(columns = [\"season\", \"date\", \"away_team_id\", \"home_team_id\", \"arena\", \"link\"])\n",
    "    months = [\"october\", \"november\", \"december\", \"january\", \"february\", \"march\", \"april\"]\n",
    "    for year in seasons:\n",
    "        print(f\"getting {year} data....\")\n",
    "        for month in months:\n",
    "            month_schedule = f\"https://www.basketball-reference.com/leagues/NBA_{year}_games-{month}.html\"\n",
    "            time.sleep(random.randint(2, 6))\n",
    "            #checks whether the link is valid and lets us know if any connections didnt work by saving to erros df. some errors here are good!\n",
    "            try:\n",
    "                month_schedule_link = urlopen(month_schedule)\n",
    "                soup = BeautifulSoup(month_schedule_link, features=\"lxml\")\n",
    "            except:\n",
    "                #the month may not have been a part of the season/year so each year will have some error months. this is really here for the covid seasons and high level error checking\n",
    "                errors = pd.DataFrame({\"season\": [year], \"month\":[month], \"error\": [\"invalid season link\"]})\n",
    "                try:\n",
    "                    name_of_file = f\"{filename}_error.parquet.gzip\"\n",
    "                    errors = pd.concat([pd.read_parquet(name_of_file), errors], ignore_index = True)\n",
    "                    errors.to_parquet(name_of_file)              \n",
    "                except:\n",
    "                    name_of_file = f\"{filename}_error.parquet.gzip\"\n",
    "                    errors.to_parquet(name_of_file)\n",
    "                continue\n",
    "            try:    \n",
    "                schedule_table = soup.find('table', attrs={'id':'schedule'})\n",
    "                games = schedule_table.tbody.findAll(\"tr\")\n",
    "                season = []\n",
    "                dates = []\n",
    "                away_team_ids = []\n",
    "                home_team_ids = []\n",
    "                arenas = []\n",
    "                links = []\n",
    "                for game in games:\n",
    "                #avoinding playoffs schedule\n",
    "                    if game.th.string == \"Playoffs\":\n",
    "                        break\n",
    "                    season.append(year)\n",
    "                    other_stats = game.findAll(\"td\")\n",
    "                    date = game.th.a.string\n",
    "                    dates.append(date)\n",
    "                    away_team = other_stats[1].a.get('href')[7:10]\n",
    "                    away_team_ids.append(away_team)\n",
    "                    home_team = other_stats[3].a.get('href')[7:10]\n",
    "                    home_team_ids.append(home_team)\n",
    "                    boxscore = other_stats[5].a.get('href')\n",
    "                    links.append(boxscore)\n",
    "                    arena = other_stats[8].string\n",
    "                    arenas.append(arena)\n",
    "                season_month = pd.DataFrame({\"season\": season,\n",
    "                                             \"date\": dates,\n",
    "                                             \"away_team_id\": away_team_ids, \n",
    "                                             \"home_team_id\": home_team_ids, \n",
    "                                              \"arena\":arenas,\n",
    "                                             \"link\":links\n",
    "                                            })\n",
    "            except:\n",
    "                #table error most likely 2020 april, where games werent played\n",
    "                errors = pd.DataFrame({\"season\": [year], \"month\":[month], \"error\": [\"unable to scrape table\"]})\n",
    "                try:\n",
    "                    name_of_file = f\"{filename}_error.parquet.gzip\"\n",
    "                    errors = pd.concat([pd.read_parquet(name_of_file), errors], ignore_index = True)\n",
    "                    errors.to_parquet(name_of_file)              \n",
    "                except:\n",
    "                    name_of_file = f\"{filename}_error.parquet.gzip\"\n",
    "                    errors.to_parquet(name_of_file)\n",
    "                continue\n",
    "            # try to append to an existing file, if not exist create it \n",
    "            try:\n",
    "                name_of_file = f\"{filename}.parquet.gzip\"\n",
    "                season_games = pd.concat([pd.read_parquet(name_of_file), season_month], ignore_index = True)\n",
    "                season_games.to_parquet(name_of_file)              \n",
    "            except:\n",
    "                name_of_file = f\"{filename}.parquet.gzip\"\n",
    "                season_month.to_parquet(name_of_file)\n",
    "            print(month, \"completed\")\n",
    "    return pd.read_parquet(name_of_file)\n",
    "\n",
    "def get_game_stats(dataframe, filename):\n",
    "    \"\"\"\n",
    "    gets individual game statistics given a seasons games as a pandas df \n",
    "    \"\"\"\n",
    "    count = 0\n",
    "    flag = 0\n",
    "    print(\"getitng individual game stats\", end = \" \")\n",
    "    for game in dataframe.iterrows():\n",
    "        print('-', end=\"\") \n",
    "        game = game[1]\n",
    "        home_team_id = game[\"home_team_id\"]\n",
    "        away_team_id = game[\"away_team_id\"]\n",
    "        arena = game[\"arena\"]\n",
    "        ext = game[\"link\"]\n",
    "        this_season = game[\"season\"]\n",
    "        # if count % 100 == 0:\n",
    "        #     if count!=0:\n",
    "        #         print(\"\\nscraped \", len(pd.read_parquet(f\"{filename}.parquet.gzip\")), f\" games from {this_season}\")\n",
    "        try:\n",
    "            box_score_url = f\"https://www.basketball-reference.com{ext}\"\n",
    "            time.sleep(random.randint(2, 6))\n",
    "            open_link = urlopen(box_score_url)\n",
    "            soup = BeautifulSoup(open_link, features=\"lxml\")\n",
    "            print(\"finished_try.............................................................\")\n",
    "        except Exception as e: \n",
    "            print(e)\n",
    "            error = pd.DataFrame({\"season\": game[\"season\"], \n",
    "            \"date\": game[\"date\"], \n",
    "            \"away_team_id\": away_team_id, \n",
    "            \"home_team_id\": home_team_id, \n",
    "            \"arena\": arena, \n",
    "            \"link\": ext, \n",
    "            \"error\": [\"404: invalid game link\"]})\n",
    "            try:\n",
    "                name_of_file = f\"{filename}_error.parquet.gzip\"\n",
    "                errors = pd.concat([pd.read_parquet(name_of_file), error], ignore_index = True)\n",
    "                errors.to_parquet(name_of_file)              \n",
    "            except:\n",
    "                name_of_file = f\"{filename}_error.parquet.gzip\"\n",
    "                error.to_parquet(name_of_file)\n",
    "            continue\n",
    "        try:\n",
    "            home_stats = soup.find('table', attrs={'id':f\"box-{home_team_id}-game-basic\"}).tfoot.tr\n",
    "            home_fg = home_stats.find('td', attrs = {'data-stat': 'fg'}).string\n",
    "            home_fga = home_stats.find('td', attrs = {'data-stat': 'fga'}).string\n",
    "            home_fg_pct = home_stats.find('td', attrs = {'data-stat': 'fg_pct'}).string\n",
    "            home_fg3 = home_stats.find('td', attrs = {'data-stat': 'fg3'}).string\n",
    "            home_fg3a = home_stats.find('td', attrs = {'data-stat': 'fg3a'}).string\n",
    "            home_fg3_pct = home_stats.find('td', attrs = {'data-stat': 'fg3_pct'}).string\n",
    "            home_ft = home_stats.find('td', attrs = {'data-stat': 'ft'}).string\n",
    "            home_fta = home_stats.find('td', attrs = {'data-stat': 'fta'}).string\n",
    "            home_ft_pct = home_stats.find('td', attrs = {'data-stat': 'ft_pct'}).string\n",
    "            home_orb = home_stats.find('td', attrs = {'data-stat': 'orb'}).string\n",
    "            home_drb = home_stats.find('td', attrs = {'data-stat': 'drb'}).string\n",
    "            home_trb = home_stats.find('td', attrs = {'data-stat': 'trb'}).string\n",
    "            home_ast = home_stats.find('td', attrs = {'data-stat': 'ast'}).string\n",
    "            home_stl = home_stats.find('td', attrs = {'data-stat': 'stl'}).string\n",
    "            home_blk = home_stats.find('td', attrs = {'data-stat': 'blk'}).string\n",
    "            home_tov = home_stats.find('td', attrs = {'data-stat': 'tov'}).string\n",
    "            home_pf = home_stats.find('td', attrs = {'data-stat': 'pf'}).string\n",
    "            home_pts = home_stats.find('td', attrs = {'data-stat': 'pts'}).string\n",
    "\n",
    "            away_stats = soup.find('table', attrs={'id':f\"box-{away_team_id}-game-basic\"}).tfoot.tr\n",
    "            away_fg = away_stats.find('td', attrs = {'data-stat': 'fg'}).string\n",
    "            away_fga = away_stats.find('td', attrs = {'data-stat': 'fga'}).string\n",
    "            away_fg_pct = away_stats.find('td', attrs = {'data-stat': 'fg_pct'}).string\n",
    "            away_fg3 = away_stats.find('td', attrs = {'data-stat': 'fg3'}).string\n",
    "            away_fg3a = away_stats.find('td', attrs = {'data-stat': 'fg3a'}).string\n",
    "            away_fg3_pct = away_stats.find('td', attrs = {'data-stat': 'fg3_pct'}).string\n",
    "            away_ft = away_stats.find('td', attrs = {'data-stat': 'ft'}).string\n",
    "            away_fta = away_stats.find('td', attrs = {'data-stat': 'fta'}).string\n",
    "            away_ft_pct = away_stats.find('td', attrs = {'data-stat': 'ft_pct'}).string\n",
    "            away_orb = away_stats.find('td', attrs = {'data-stat': 'orb'}).string\n",
    "            away_drb = away_stats.find('td', attrs = {'data-stat': 'drb'}).string\n",
    "            away_trb = away_stats.find('td', attrs = {'data-stat': 'trb'}).string\n",
    "            away_ast = away_stats.find('td', attrs = {'data-stat': 'ast'}).string\n",
    "            away_stl = away_stats.find('td', attrs = {'data-stat': 'stl'}).string\n",
    "            away_blk = away_stats.find('td', attrs = {'data-stat': 'blk'}).string\n",
    "            away_tov = away_stats.find('td', attrs = {'data-stat': 'tov'}).string\n",
    "            away_pf = away_stats.find('td', attrs = {'data-stat': 'pf'}).string\n",
    "            away_pts = away_stats.find('td', attrs = {'data-stat': 'pts'}).string\n",
    "\n",
    "            game_stats = pd.DataFrame({\n",
    "            \"home_team\":[home_team_id],\n",
    "            \"home_fg\":[home_fg], \n",
    "            \"home_fga\":[home_fga], \n",
    "            \"home_fg_pct\":[home_fg_pct], \n",
    "            \"home_fg3\":[home_fg3], \n",
    "            \"home_fg3a\":[home_fg3a], \n",
    "            \"home_fg3_pct\":[home_fg3_pct], \n",
    "            \"home_ft\":[home_ft], \n",
    "            \"home_fta\":[home_fta], \n",
    "            \"home_ft_pct\":[home_ft_pct], \n",
    "            \"home_orb\":[home_orb], \n",
    "            \"home_drb\":[home_drb],\n",
    "            \"home_trb\":[home_trb],\n",
    "            \"home_ast\":[home_ast],\n",
    "            \"home_stl\":[home_stl],\n",
    "            \"home_blk\":[home_blk],\n",
    "            \"home_tov\":[home_tov],\n",
    "            \"home_pf\":[home_pf],\n",
    "            \"home_pts\":[home_pts],\n",
    "            \"away_team\":[away_team_id],\n",
    "            \"away_fg\":[away_fg], \n",
    "            \"away_fga\":[away_fga], \n",
    "            \"away_fg_pct\":[away_fg_pct], \n",
    "            \"away_fg3\":[away_fg3], \n",
    "            \"away_fg3a\":[away_fg3a], \n",
    "            \"away_fg3_pct\":[away_fg3_pct], \n",
    "            \"away_ft\":[away_ft], \n",
    "            \"away_fta\":[away_fta], \n",
    "            \"away_ft_pct\":[away_ft_pct], \n",
    "            \"away_orb\":[away_orb], \n",
    "            \"away_drb\":[away_drb],\n",
    "            \"away_trb\":[away_trb],\n",
    "            \"away_ast\":[away_ast],\n",
    "            \"away_stl\":[away_stl],\n",
    "            \"away_blk\":[away_blk],\n",
    "            \"away_tov\":[away_tov],\n",
    "            \"away_pf\":[away_pf],\n",
    "            \"away_pts\":[away_pts],\n",
    "            \"arena\": [arena]})\n",
    "            \n",
    "        except:\n",
    "            \n",
    "            error = pd.DataFrame({\"season\": game[\"season\"], \n",
    "            \"date\": game[\"date\"], \n",
    "            \"away_team_id\": away_team_id, \n",
    "            \"home_team_id\": home_team_id, \n",
    "            \"arena\": arena, \n",
    "            \"link\": ext, \n",
    "            \"error\": [\"error scraping table\"]})\n",
    "            flag = 1\n",
    "            print(\"error\", error)\n",
    "            try:\n",
    "                name_of_file = f\"{filename}_error.parquet.gzip\"\n",
    "                errors = pd.concat([pd.read_parquet(name_of_file), error], ignore_index = True)\n",
    "                errors.to_parquet(name_of_file)              \n",
    "            except:\n",
    "                name_of_file = f\"{filename}_error.parquet.gzip\"\n",
    "                error.to_parquet(name_of_file)\n",
    "            continue\n",
    "            \n",
    "        if count % 100 == 0 and count != 0:\n",
    "            try:\n",
    "                name_of_file = f\"{filename}.parquet.gzip\"\n",
    "                game_stats = pd.concat([pd.read_parquet(name_of_file), game_stats], ignore_index = True)\n",
    "                game_stats.to_parquet(name_of_file)\n",
    "                print(f\"\\nteam data for {len(game_stats)} teams collected in the {this_season}\")\n",
    "            except:\n",
    "                name_of_file = f\"{filename}.parquet.gzip\"\n",
    "                game_stats.to_parquet(name_of_file)\n",
    "        count+=1\n",
    "    try:\n",
    "        name_of_file = f\"{filename}.parquet.gzip\"\n",
    "        game_stats = pd.concat([pd.read_parquet(name_of_file), game_stats], ignore_index = True)\n",
    "        game_stats.to_parquet(name_of_file)             \n",
    "    except:\n",
    "        name_of_file = f\"{filename}.parquet.gzip\"\n",
    "        game_stats.to_parquet(name_of_file) \n",
    "    if flag == 1:\n",
    "        print(\"errors: \", len(pd.read_parquet(f\"{filename}_error.parquet.gzip\")))\n",
    "    return game_stats\n",
    "\n",
    "\n",
    "def main():\n",
    "    my_vol = int(input(\"volume number:\"))\n",
    "    my_vol-=1\n",
    "    \n",
    "    Volume_1 =  [2015, 2016]\n",
    "    Volume_2 =  [2017, 2018]\n",
    "    Volume_3 =  [2019, 2020]\n",
    "    Volume_4 =  [2021, 2022]\n",
    "    \n",
    "    volumes = [Volume_1, Volume_2, Volume_3, Volume_4]\n",
    "    print(\"getting seasons: \", str(volumes[my_vol]))\n",
    "    \n",
    "    \n",
    "    #gets game schedule of a list of seasons\n",
    "    # get_season_schedule(str(volumes[my_vol]), volumes[my_vol])\n",
    "    name_of_file = f\"{str(volumes[my_vol])}.parquet.gzip\"\n",
    "    season_games = pd.read_parquet(name_of_file)\n",
    "    get_game_stats(season_games, f\"{str(volumes[my_vol])}_game_stats\")\n",
    "    print(\"2-4 .parquet.gzip files should be saved in the current directory!\")\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>season</th>\n",
       "      <th>date</th>\n",
       "      <th>away_team_id</th>\n",
       "      <th>home_team_id</th>\n",
       "      <th>arena</th>\n",
       "      <th>link</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2021</td>\n",
       "      <td>Tue, Dec 22, 2020</td>\n",
       "      <td>GSW</td>\n",
       "      <td>BRK</td>\n",
       "      <td>Barclays Center</td>\n",
       "      <td>/boxscores/202012220BRK.html</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2021</td>\n",
       "      <td>Tue, Dec 22, 2020</td>\n",
       "      <td>LAC</td>\n",
       "      <td>LAL</td>\n",
       "      <td>STAPLES Center</td>\n",
       "      <td>/boxscores/202012220LAL.html</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2021</td>\n",
       "      <td>Wed, Dec 23, 2020</td>\n",
       "      <td>CHO</td>\n",
       "      <td>CLE</td>\n",
       "      <td>Rocket Mortgage Fieldhouse</td>\n",
       "      <td>/boxscores/202012230CLE.html</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2021</td>\n",
       "      <td>Wed, Dec 23, 2020</td>\n",
       "      <td>NYK</td>\n",
       "      <td>IND</td>\n",
       "      <td>Bankers Life Fieldhouse</td>\n",
       "      <td>/boxscores/202012230IND.html</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2021</td>\n",
       "      <td>Wed, Dec 23, 2020</td>\n",
       "      <td>MIA</td>\n",
       "      <td>ORL</td>\n",
       "      <td>Amway Center</td>\n",
       "      <td>/boxscores/202012230ORL.html</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8891</th>\n",
       "      <td>2022</td>\n",
       "      <td>Wed, Apr 27, 2022</td>\n",
       "      <td>DEN</td>\n",
       "      <td>GSW</td>\n",
       "      <td>Chase Center</td>\n",
       "      <td>/boxscores/202204270GSW.html</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8892</th>\n",
       "      <td>2022</td>\n",
       "      <td>Thu, Apr 28, 2022</td>\n",
       "      <td>PHI</td>\n",
       "      <td>TOR</td>\n",
       "      <td>Scotiabank Arena</td>\n",
       "      <td>/boxscores/202204280TOR.html</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8893</th>\n",
       "      <td>2022</td>\n",
       "      <td>Thu, Apr 28, 2022</td>\n",
       "      <td>PHO</td>\n",
       "      <td>NOP</td>\n",
       "      <td>Smoothie King Center</td>\n",
       "      <td>/boxscores/202204280NOP.html</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8894</th>\n",
       "      <td>2022</td>\n",
       "      <td>Thu, Apr 28, 2022</td>\n",
       "      <td>DAL</td>\n",
       "      <td>UTA</td>\n",
       "      <td>Vivint Smart Home Arena</td>\n",
       "      <td>/boxscores/202204280UTA.html</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8895</th>\n",
       "      <td>2022</td>\n",
       "      <td>Fri, Apr 29, 2022</td>\n",
       "      <td>MEM</td>\n",
       "      <td>MIN</td>\n",
       "      <td>Target Center</td>\n",
       "      <td>/boxscores/202204290MIN.html</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8896 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      season               date away_team_id home_team_id  \\\n",
       "0       2021  Tue, Dec 22, 2020          GSW          BRK   \n",
       "1       2021  Tue, Dec 22, 2020          LAC          LAL   \n",
       "2       2021  Wed, Dec 23, 2020          CHO          CLE   \n",
       "3       2021  Wed, Dec 23, 2020          NYK          IND   \n",
       "4       2021  Wed, Dec 23, 2020          MIA          ORL   \n",
       "...      ...                ...          ...          ...   \n",
       "8891    2022  Wed, Apr 27, 2022          DEN          GSW   \n",
       "8892    2022  Thu, Apr 28, 2022          PHI          TOR   \n",
       "8893    2022  Thu, Apr 28, 2022          PHO          NOP   \n",
       "8894    2022  Thu, Apr 28, 2022          DAL          UTA   \n",
       "8895    2022  Fri, Apr 29, 2022          MEM          MIN   \n",
       "\n",
       "                           arena                          link  \n",
       "0                Barclays Center  /boxscores/202012220BRK.html  \n",
       "1                 STAPLES Center  /boxscores/202012220LAL.html  \n",
       "2     Rocket Mortgage Fieldhouse  /boxscores/202012230CLE.html  \n",
       "3        Bankers Life Fieldhouse  /boxscores/202012230IND.html  \n",
       "4                   Amway Center  /boxscores/202012230ORL.html  \n",
       "...                          ...                           ...  \n",
       "8891                Chase Center  /boxscores/202204270GSW.html  \n",
       "8892            Scotiabank Arena  /boxscores/202204280TOR.html  \n",
       "8893        Smoothie King Center  /boxscores/202204280NOP.html  \n",
       "8894     Vivint Smart Home Arena  /boxscores/202204280UTA.html  \n",
       "8895               Target Center  /boxscores/202204290MIN.html  \n",
       "\n",
       "[8896 rows x 6 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd \n",
    "pd.read_parquet(\"[2021, 2022].parquet 2.gzip\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '[2021, 2022]_game_stats_error.parquet.gzip'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m pd\u001b[38;5;241m.\u001b[39mread_parquet(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[2021, 2022]_game_stats_error.parquet.gzip\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/io/parquet.py:503\u001b[0m, in \u001b[0;36mread_parquet\u001b[0;34m(path, engine, columns, storage_options, use_nullable_dtypes, **kwargs)\u001b[0m\n\u001b[1;32m    456\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    457\u001b[0m \u001b[38;5;124;03mLoad a parquet object from the file path, returning a DataFrame.\u001b[39;00m\n\u001b[1;32m    458\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    499\u001b[0m \u001b[38;5;124;03mDataFrame\u001b[39;00m\n\u001b[1;32m    500\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    501\u001b[0m impl \u001b[38;5;241m=\u001b[39m get_engine(engine)\n\u001b[0;32m--> 503\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m impl\u001b[38;5;241m.\u001b[39mread(\n\u001b[1;32m    504\u001b[0m     path,\n\u001b[1;32m    505\u001b[0m     columns\u001b[38;5;241m=\u001b[39mcolumns,\n\u001b[1;32m    506\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39mstorage_options,\n\u001b[1;32m    507\u001b[0m     use_nullable_dtypes\u001b[38;5;241m=\u001b[39muse_nullable_dtypes,\n\u001b[1;32m    508\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    509\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/io/parquet.py:244\u001b[0m, in \u001b[0;36mPyArrowImpl.read\u001b[0;34m(self, path, columns, use_nullable_dtypes, storage_options, **kwargs)\u001b[0m\n\u001b[1;32m    241\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m manager \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marray\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    242\u001b[0m     to_pandas_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msplit_blocks\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m  \u001b[38;5;66;03m# type: ignore[assignment]\u001b[39;00m\n\u001b[0;32m--> 244\u001b[0m path_or_handle, handles, kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfilesystem\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m _get_path_or_handle(\n\u001b[1;32m    245\u001b[0m     path,\n\u001b[1;32m    246\u001b[0m     kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfilesystem\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m    247\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39mstorage_options,\n\u001b[1;32m    248\u001b[0m     mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    249\u001b[0m )\n\u001b[1;32m    250\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    251\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapi\u001b[38;5;241m.\u001b[39mparquet\u001b[38;5;241m.\u001b[39mread_table(\n\u001b[1;32m    252\u001b[0m         path_or_handle, columns\u001b[38;5;241m=\u001b[39mcolumns, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    253\u001b[0m     )\u001b[38;5;241m.\u001b[39mto_pandas(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mto_pandas_kwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/io/parquet.py:102\u001b[0m, in \u001b[0;36m_get_path_or_handle\u001b[0;34m(path, fs, storage_options, mode, is_dir)\u001b[0m\n\u001b[1;32m     92\u001b[0m handles \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m     94\u001b[0m     \u001b[38;5;129;01mnot\u001b[39;00m fs\n\u001b[1;32m     95\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_dir\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[38;5;66;03m# fsspec resources can also point to directories\u001b[39;00m\n\u001b[1;32m    101\u001b[0m     \u001b[38;5;66;03m# this branch is used for example when reading from non-fsspec URLs\u001b[39;00m\n\u001b[0;32m--> 102\u001b[0m     handles \u001b[38;5;241m=\u001b[39m get_handle(\n\u001b[1;32m    103\u001b[0m         path_or_handle, mode, is_text\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, storage_options\u001b[38;5;241m=\u001b[39mstorage_options\n\u001b[1;32m    104\u001b[0m     )\n\u001b[1;32m    105\u001b[0m     fs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    106\u001b[0m     path_or_handle \u001b[38;5;241m=\u001b[39m handles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/io/common.py:865\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    856\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[1;32m    857\u001b[0m             handle,\n\u001b[1;32m    858\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    861\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    862\u001b[0m         )\n\u001b[1;32m    863\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    864\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m--> 865\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n\u001b[1;32m    866\u001b[0m     handles\u001b[38;5;241m.\u001b[39mappend(handle)\n\u001b[1;32m    868\u001b[0m \u001b[38;5;66;03m# Convert BytesIO or file objects passed with an encoding\u001b[39;00m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '[2021, 2022]_game_stats_error.parquet.gzip'"
     ]
    }
   ],
   "source": [
    "pd.read_parquet(\"[2021, 2022]_game_stats_error.parquet.gzip\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
